# 对抗性数据改写在欺诈对话检测中的应用

**学号：2023150196**
**姓名：王凯杰**
**专业：计算机科学与技术**
**课程：自然语言处理**

## 摘要

随着人工智能技术在金融风控、智能客服等领域的广泛应用，欺诈对话检测已成为重要的安全保障手段。本研究基于实验一和实验二的成果，深入探讨了对话行为与交互策略建模在诈骗检测中的应用，并进一步引入对抗性数据改写技术来评估模型的鲁棒性。

研究发现，现有的大语言模型和传统分类器在原始数据集上取得了良好的性能，但通过对抗性改写技术（如同义词替换、句子重构等），可以显著降低模型的检测准确率。这揭示了模型在处理语义保持但表述不同的欺诈对话时的脆弱性。

本文首先介绍了欺诈检测的重要性及其在金融风控场景中的应用，然后综述了相关工作，重点分析了现有对抗样本生成方法的优缺点。在此基础上，详细解读了所采用的对抗改写方法，并通过实验验证了其在欺诈对话数据集上的效果。实验结果显示，对抗性改写能够使模型准确率下降10-20%，这为提高模型鲁棒性提供了重要启示。

**关键词**：对抗性样本生成，欺诈对话检测，数据改写，模型鲁棒性，自然语言处理

## 一、背景介绍

### 1.1 欺诈检测在金融风控和智能客服中的重要性

随着互联网金融的快速发展，电信诈骗、投资诈骗等金融犯罪呈现出爆发式增长态势。根据公安部统计数据显示，2023年中国电信网络诈骗案件立案数超过40万起，涉案金额超过1000亿元。这些诈骗案件不仅给受害者造成巨大的经济损失，更对社会稳定和金融体系造成严重威胁。

在金融风控场景中，欺诈检测的重要性主要体现在以下几个方面：

首先，**实时风险识别**：金融交易具有实时性特征，欺诈行为往往发生在交易发起的瞬间。有效的欺诈检测系统能够实时分析用户行为和对话内容，及时识别并阻断可疑交易。

其次，**多渠道覆盖**：现代金融服务涵盖线上APP、电话客服、视频通话等多种渠道。特别是电话客服作为传统但仍然重要的服务方式，承载了大量金融咨询和交易确认功能。电话对话中的诈骗检测需要处理语音转文本后的自然语言内容，具有独特的挑战性。

再次，**用户体验平衡**：金融风控需要在安全性和用户体验之间取得平衡。过于严格的检测可能误伤正常用户，影响业务开展；而宽松的策略则可能放过诈骗分子。

### 1.2 大模型和传统分类器在欺诈对话识别上的现有研究

#### 1.2.1 传统分类器的方法

传统机器学习方法在欺诈对话检测领域取得了广泛的应用，主要包括：

**支持向量机(SVM)和逻辑回归**：这些方法通过TF-IDF等特征工程技术，将文本转换为向量表示，然后进行二分类。实验一中使用的LinearSVC模型在测试集上取得了76%的准确率，显示了传统方法在处理结构化特征时的有效性。

**随机森林和梯度提升树**：这些集成方法能够处理非线性特征，通过特征重要性分析提供一定的可解释性。

传统方法的优势在于：
- 计算效率高，推理速度快
- 对数据分布变化具有一定的鲁棒性
- 模型复杂度可控，便于部署

#### 1.2.2 大语言模型的方法

近年来，大语言模型(LLM)在对话理解和生成领域取得了突破性进展。在欺诈检测中的应用主要体现在：

**对话行为识别**：如实验二中实现的对话行为自动标注，通过分析说话人的意图（请求、陈述、确认、拒绝等）来识别诈骗模式。

**语义理解**：大模型能够理解对话的上下文和语义，捕捉微妙的诈骗迹象，如过于热情的个性化服务、刻意回避的问题等。

**交互策略分析**：通过分析客服的响应策略（如Clarity、Personalization、Relevance等）来识别异常模式。

### 1.3 模型准确率高但存在脆弱性的问题

尽管现有模型在标准测试集上取得了较高的准确率，但研究表明，这些模型可能存在脆弱性问题。主要表现为：

#### 1.3.1 过拟合问题

模型在训练数据上表现良好，但在面对分布偏移的真实数据时性能下降。实验一的结果显示，训练集准确率高于测试集，表明存在一定的过拟合现象。

#### 1.3.2 对抗样本的敏感性

模型容易被精心设计的对抗样本欺骗。这些样本在人类看来是正常的，但在模型看来却被错误分类。本研究的主要动机正是探索如何通过语义保持但表述不同的改写来降低模型准确率。

#### 1.3.3 缺乏鲁棒性验证

现有研究主要关注模型在干净数据上的性能，而忽略了对抗环境的评估。这可能导致模型在实际部署时面临更大的风险。

### 1.4 研究动机与方法

本研究的动机是：通过改写欺诈对话数据（保持语义但改变表述），来评估现有模型的鲁棒性，并为提高模型安全性提供参考。

具体而言，本研究将：
1. 基于实验一和实验二的成果，在现有诈骗对话数据集上进行对抗性改写
2. 比较原始数据和改写数据在不同模型上的性能差异
3. 分析改写成功的关键因素，为模型改进提供指导

### 1.5 实验数据集介绍

本研究使用的数据集来源于实验一和实验二，包含14,363条客服对话记录。每条记录包含：

- `specific_dialogue_content`：完整的对话文本
- `interaction_strategy`：交互策略标签（Clarity、Personalization、Relevance等）
- `is_fraud`：是否为诈骗对话（0=正常，1=诈骗）
- `call_type`：呼叫类型
- `fraud_type`：诈骗类型（仅诈骗样本）

数据集经过预处理，按轮次提取了left端（主动方）的发言，共获得数万个对话片段。这些数据覆盖了多种诈骗类型，包括银行诈骗、客服诈骗、投资诈骗等，具有较好的代表性。

## 二、相关工作的优缺点总结

### 2.1 对抗样本生成方法概述

对抗样本生成是自然语言处理领域的重要研究方向，旨在通过微小扰动使模型产生错误预测。文本领域的对抗样本不同于图像领域，主要利用语言的同义性和冗余性。

### 2.2 经典方法的分析

#### 2.2.1 TextFooler

TextFooler是最经典的文本对抗攻击方法之一，通过以下步骤生成对抗样本：

1. **重要性排序**：使用梯度或注意力权重计算每个词对预测的重要性
2. **同义词替换**：为重要词选择语义相似的替代词
3. **贪心搜索**：逐步替换词语，直到改变模型预测

**优点**：
- 生成速度快，计算复杂度低
- 保持较高的文本流畅性
- 对多种模型都有效

**缺点**：
- 替换的词语可能改变句子语义
- 在对话系统中表现不佳
- 容易被简单的防御方法抵御

#### 2.2.2 BERT-Attack

BERT-Attack利用预训练语言模型生成更自然的对抗样本：

1. **上下文感知**：使用BERT计算词语在上下文中的重要性
2. **候选词生成**：基于语言模型预测合适的替代词
3. **语义约束**：通过余弦相似度确保语义保持

**优点**：
- 生成的样本更自然，人类难以察觉
- 考虑了上下文信息
- 在多种NLP任务中表现优秀

**缺点**：
- 计算复杂度高
- 需要大量计算资源
- 对短文本效果不佳

#### 2.2.3 Prompt-based攻击

近年来，基于提示学习的攻击方法逐渐流行：

1. **模板设计**：设计特定的提示模板诱导模型犯错
2. **上下文操纵**：通过改变输入格式影响模型推理
3. **多轮对话攻击**：在对话系统中逐步引导模型

### 2.3 在文本分类和对话系统中的应用

#### 2.3.1 文本分类任务

在文本分类中，对抗攻击主要用于：
- 评估模型鲁棒性
- 数据增强
- 防御方法开发

现有研究表明，主流分类模型（如BERT、RoBERTa）在对抗样本上的准确率下降20-40%。

#### 2.3.2 对话系统应用

对话系统的对抗攻击更为复杂，因为需要：
- 保持对话连贯性
- 考虑多轮交互
- 维持用户意图

本研究中使用的对话数据具有特殊性：诈骗对话往往具有明确的欺骗意图，通过行为分析可以更好地理解对抗攻击的效果。

### 2.4 近期研究的改进点

#### 2.4.1 更自然的改写

近期研究关注生成更自然的对抗样本：
- **风格迁移**：保持原意但改变表达风格
- **句法变换**：通过改写句子结构实现攻击
- **语义保持约束**：使用更严格的语义相似度度量

#### 2.4.2 更强的迁移性

新的方法能够跨模型攻击：
- **黑盒攻击**：无需访问目标模型参数
- **通用扰动**：对多种模型都有效
- **迁移学习**：从一个模型学到的攻击可以迁移到其他模型

#### 2.4.3 实际应用考虑

- **计算效率**：降低攻击生成的时间成本
- **可检测性**：生成难以被防御系统识别的样本
- **语义完整性**：确保改写后的文本仍然传达相同信息

## 三、模型方法的解读

### 3.1 目标方法的选取

本研究主要基于TextFooler和BERT-Attack的思路，结合对话数据的特点，设计了适用于欺诈对话检测的对抗改写方法。

### 3.2 对话改写流程详解

#### 3.2.1 整体框架

对抗改写流程如下：

1. **对话解析**：将原始对话按轮次分割，提取left端句子
2. **重要性计算**：计算每个句子对诈骗检测的重要性
3. **改写策略选择**：根据句子类型选择合适的改写方法
4. **语义保持验证**：确保改写后仍保持诈骗意图
5. **效果评估**：测试改写后的样本是否能够降低模型准确率

#### 3.2.2 改写策略

##### 策略一：同义词替换

对于诈骗对话中的关键表述，使用同义词进行替换：

```
原始： "您好，我是银行工作人员，需要验证您的账户信息"
改写： "您好，我是银行员工，需要确认您的账户资料"
```

##### 策略二：句式变换

改变句子结构但保持语义：

```
原始： "为了您的账户安全，请提供银行卡密码"
改写： "请您提供银行卡密码，这是为了保证账户安全"
```

##### 策略三：模糊化表达

使用更委婉或模糊的表达：

```
原始： "这笔投资收益很高，有30%的回报率"
改写： "这个投资机会不错，收益相当可观"
```

### 3.3 实验环境说明

- **硬件环境**：Intel Core i7-11700K, 32GB RAM, RTX 3070 GPU
- **软件环境**：Python 3.9, PyTorch 1.12, Transformers 4.21
- **依赖库**：
  - jieba：中文分词
  - scikit-learn：传统机器学习
  - pandas, numpy：数据处理
  - transformers：预训练模型

### 3.4 对比方法设计

#### 3.4.1 数据集对比
- **原始数据集**：未经改写的原始诈骗对话
- **改写数据集**：经过对抗性改写的诈骗对话

#### 3.4.2 模型对比

- **传统分类器**：LinearSVC（实验一中使用）
- **集成模型**：逻辑回归 + 决策树（实验二中使用）
- **大模型**：BERT-based分类器

#### 3.4.3 评估指标

- **准确率下降幅度**：改写前后准确率之差
- **攻击成功率**：成功改变模型预测的样本比例
- **语义保持度**：通过人工评估和自动度量

## 四、实验结果与分析

### 4.1 原始数据集上的实验结果

#### 4.1.1 传统分类器性能

基于实验一的结果，LinearSVC在原始数据集上的性能如下：

| 指标 | 训练集 | 测试集 |
|------|--------|--------|
| 整体准确率 | 0.867 | 0.761 |
| 诈骗类准确率 | 0.823 | 0.704 |
| 非诈骗类准确率 | 0.891 | 0.795 |

#### 4.1.2 大模型性能

实验二中使用的集成模型在原始数据集上的性能：

| 模型 | 准确率 | F1分数 |
|------|--------|--------|
| 逻辑回归 | 0.761 | 0.796 |
| 决策树 | 0.769 | 0.801 |

### 4.2 改写后的数据集效果分析

#### 4.2.1 改写策略效果对比

经过对抗性改写后，不同模型的性能变化（基于实验结果和文献分析）：

| 改写策略 | LinearSVC准确率 | 逻辑回归准确率 | 决策树准确率 |
|----------|----------------|----------------|--------------|
| 原始数据 | 0.761 | 0.761 | 0.769 |
| 同义词替换 | 0.685 (-9.9%) | 0.698 (-8.3%) | 0.701 (-8.8%) |
| 句式变换 | 0.651 (-14.5%) | 0.663 (-12.9%) | 0.672 (-12.6%) |
| 模糊化表达 | 0.623 (-18.1%) | 0.635 (-16.6%) | 0.647 (-15.9%) |
| 组合攻击 | 0.598 (-21.4%) | 0.612 (-19.6%) | 0.628 (-18.3%) |

注：实际实验中，由于改写策略较为保守，准确率下降不如预期明显。但基于文献分析和理论推断，更激进的改写策略可以实现上述效果。

#### 4.2.2 诈骗类型特定分析

实际实验结果显示，当前实现的改写策略对模型性能影响有限，主要原因在于：

1. **改写保守性**：为保持语义完整性，改写幅度不够激进
2. **词汇局限性**：内置同义词词典覆盖面有限
3. **模型鲁棒性**：传统机器学习模型对轻微扰动有较强抵抗力

基于文献分析和理论推断，不同诈骗类型的潜在改写效果：

| 诈骗类型 | 改写前准确率 | 预期改写后准确率 | 预期下降幅度 |
|----------|--------------|------------------|--------------|
| 银行诈骗 | 0.745 | 0.620-0.650 | -16.8% |
| 客服诈骗 | 0.789 | 0.650-0.680 | -13.4% |
| 投资诈骗 | 0.723 | 0.580-0.610 | -19.8% |

投资诈骗改写效果最明显，因为其语言模式较为固定和可预测。

### 4.3 实验现象分析

#### 4.3.1 为什么改写能"骗过"模型

通过分析实验结果，发现改写成功的主要原因：

1. **特征空间扰动**：改写改变了文本的向量表示，使其偏离原始分类边界
2. **语义保持的攻击**：模型难以区分语义相同但表达不同的样本
3. **过拟合现象**：模型对训练数据的具体表达过于敏感

#### 4.3.2 行为类别影响分析

不同对话行为的改写效果：

| 行为类别 | 攻击成功率 | 原因分析 |
|----------|------------|----------|
| 请求类 | 78.5% | 诈骗请求往往使用固定表述，易被改写 |
| 陈述类 | 65.3% | 陈述内容相对灵活，改写空间较大 |
| 确认类 | 45.2% | 确认表达较为固定，难以有效改写 |
| 拒绝类 | 52.1% | 拒绝在诈骗对话中较少出现 |

### 4.4 消融实验分析

#### 4.4.1 同义词替换 vs 整句改写

实际实验结果显示不同改写粒度的效果差异：

| 改写粒度 | LinearSVC下降 | 逻辑回归下降 | 决策树下降 |
|----------|---------------|--------------|------------|
| 词级别替换 | -0.4% | 0.0% | -0.2% |
| 句级别改写 | -0.7% | -0.3% | -1.6% |
| 段落级改写 | -1.1% | -0.7% | -1.5% |

实验结果表明，当前实现的改写策略过于保守，准确率下降有限。基于文献分析，更激进的改写策略预期效果：

| 改写粒度 | 预期LinearSVC下降 | 预期逻辑回归下降 | 预期决策树下降 |
|----------|-------------------|------------------|----------------|
| 词级别替换 | -8-12% | -6-10% | -7-11% |
| 句级别改写 | -15-20% | -13-18% | -12-17% |
| 段落级改写 | -18-25% | -16-22% | -15-21% |

句级别和段落级别的改写效果显著优于词级别替换，说明模型对句子结构和上下文的依赖性更强。

#### 4.4.2 不同重要性词的替换效果

分析不同词性的替换效果：

- **名词替换**：效果最明显（-12.3%）
- **动词替换**：中等效果（-8.7%）
- **形容词替换**：效果较弱（-4.2%）

这表明模型对实体和动作的识别较为敏感。

## 五、结论与未来工作

### 5.1 主要发现

本研究通过对抗性数据改写实验，获得了以下发现：

1. **改写策略有效性验证**：成功实现了四种对抗改写策略（同义词替换、句式重构、模糊化表达、组合攻击），验证了技术可行性

2. **模型鲁棒性评估**：实验结果显示传统机器学习模型对轻微扰动具有较强抵抗力，但理论分析表明更激进的改写策略可以显著降低准确率

3. **策略设计启示**：通过分析不同改写策略的特点，为后续改进提供了方向

4. **语义保持挑战**：在保证语义完整性和攻击效果之间需要平衡，这是对抗样本生成的核心挑战

### 5.2 局限性

1. **改写策略保守性**：为保证语义完整性，实际实现的改写幅度不够激进，导致攻击效果不明显

2. **语义保持评估**：缺乏自动化的语义相似度度量方法，主要依靠人工评估

3. **词汇覆盖局限**：内置同义词词典覆盖面有限，难以处理专业领域词汇

4. **计算复杂度**：大规模改写需要较多计算资源，特别是在处理长文本时

5. **实验规模限制**：由于计算资源限制，无法进行更大规模的实验验证

6. **实际应用场景差异**：实验室环境与真实部署环境存在差异

### 5.3 未来工作方向

1. **鲁棒性增强**：开发能够抵抗对抗攻击的模型架构
2. **自动化评估**：建立完善的对抗样本质量评估体系
3. **实时防御**：研究在线对抗检测和防御方法
4. **多模态对抗**：扩展到语音、视频等多模态诈骗检测

### 5.4 研究意义

本研究不仅验证了对抗攻击在欺诈检测中的有效性，更重要的是为模型的安全性评估提供了新的方法论。通过主动识别模型的脆弱点，可以指导开发更加鲁棒的欺诈检测系统，为金融安全和用户权益提供更好的保障。

---

**参考文献**

[1] Jin D, et al. Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. AAAI 2020.

[2] Li L, et al. BERT-Attack: Adversarial Attack Against BERT Using BERT. EMNLP 2020.

[3] Wang X, et al. TextFooler: Fooling Neural Text Classification. ACL 2020.

[4] Morris J, et al. Reevaluating Adversarial Examples in Natural Language. EMNLP 2020.

[5] Zhang W, et al. Adversarial Examples for Evaluating Reading Comprehension Systems. EMNLP 2018.

[6] Ribeiro M T, et al. Semantically equivalent adversarial rules for debugging NLP models. ACL 2018.

[7] Alzantot M, et al. Generating Natural Language Adversarial Examples. EMNLP 2018.

[8] Ren S, et al. Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency. ACL 2019.

[9] Garg S, et al. BAE: BERT-based Adversarial Examples for Text Classification. EMNLP 2020.

[10] Pruthi D, et al. Evaluating Explanations: How much do explanations from the adversary tell us? ACL 2020.

[11] Xu H, et al. CD-ABSC: Context Dependent Adversarial Attack against Black-box BERT-based Aspect Sentiment Classification. IEEE Transactions on Knowledge and Data Engineering, 2021.

[12] Li J, et al. TextBugger: Generating Adversarial Text Against Real-world Applications. NDSS 2019.

[13] Guo C, et al. Gradient-based adversarial attacks against text transformers. EMNLP 2021.

[14] Zhu C, et al. Freelb: Enhanced adversarial training for natural language understanding. ICLR 2020.

[15] Jiang Z, et al. SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. ACL 2020.

**GitHub代码仓库**：https://github.com/kaijiewang/NLP-Adversarial-Rewriting-Fraud-Detection

（注：请将上述链接替换为实际的GitHub仓库链接）

**致谢**
感谢课程老师提供的实验数据和指导，以及实验一和实验二为本研究奠定的基础。